# ğŸ½ï¸ Open Food Facts API - Laravel Challenge TruckPag

API para importaÃ§Ã£o, armazenamento e gerenciamento de dados alimentÃ­cios com base na base de dados pÃºblica [Open Food Facts](https://br.openfoodfacts.org/data).
---
This is a challenge by Coodesh


## ğŸ“š Sobre o Projeto

Este projeto foi desenvolvido como parte de um desafio tÃ©cnico proposto pela **TruckPag**. Ele consiste em consumir dados do Open Food Facts, armazenÃ¡-los em uma base MongoDB e expÃ´-los via uma API Laravel.

---
## ğŸ“„ DocumentaÃ§Ã£o da API

A documentaÃ§Ã£o da API estÃ¡ disponÃ­vel via Swagger UI:

ğŸ”— **[Clique aqui para acessar a documentaÃ§Ã£o](http://localhost:8000/api/documentation)**
**A aplicaÃ§Ã£o precisa estar rodando para acessar a documentaÃ§Ã£o**

## ğŸš€ Tecnologias Utilizadas

### ğŸ§± Back-End

- **Linguagem**: PHP 8.2  
- **Framework**: Laravel 10  
- **Banco de Dados**: MongoDB (MongoDB Atlas)  
- **Gerenciador de DependÃªncias**: Composer  

### ğŸ“¦ Bibliotecas e Pacotes

- [`mongodb/mongodb`](https://github.com/mongodb/mongo-php-library) â€” Driver MongoDB para PHP  
- Laravel HTTP Client (baseado em Guzzle) â€” para requisiÃ§Ãµes aos arquivos `.json.gz`  


### ğŸ› ï¸ Ferramentas

- **Docker Compose** â€” para containerizaÃ§Ã£o do ambiente e orquestraÃ§Ã£o de serviÃ§os  
- **Supervisor** â€” gerenciamento de mÃºltiplos processos simultÃ¢neos (API, queue worker, scheduler)  
- **Postman** â€” testes manuais da API  
- **Git** â€” versionamento do projeto

## ğŸš€ Como Instalar e Usar o Projeto

VocÃª pode rodar este projeto **com ou sem Docker**. Abaixo estÃ£o as instruÃ§Ãµes para ambos os cenÃ¡rios:

---

### âœ… Rodando **sem Docker**

```bash
# Clone o repositÃ³rio e entre na pasta
git clone https://github.com/josesousacruz/products-parse-truckPag.git
cd food-api-truckpag

# Instale as dependÃªncias
composer install

# Configure o MongoDB no .env

# Em terminais separados, execute:

# Terminal 1 - para rodar o projeto
php artisan serve

# Terminal 2 - para processar a fila
php artisan queue:work

# Terminal 3 - para executar os agendamentos a cada minuto
php artisan schedule:work
```
---

### ğŸ³ Rodando **com Docker**

```bash
# Acesse a pasta docker/
cd docker

# Suba os containers
docker compose up --build -d
```

Esse comando irÃ¡:
- Subir o Laravel com PHP
- Iniciar o servidor embutido
- Executar o scheduler e o queue worker via Supervisor

- 
> â° O comando agendado para importar os arquivos do Open Food Facts roda automaticamente todos os dias Ã s **02:00 da manhÃ£**.
> Ou Execute:
```bash
docker exec -it nome-do-container php artisan import:openfoodfacts
```
## ğŸ§ª Como Executar os Testes

```bash
# Sem Docker
php artisan test

# Com Docker
docker exec -it nome-do-container php artisan test

```

## ğŸ› ï¸ Processo de Desenvolvimento

### DecisÃ£o pelo Banco de Dados  
Considerei utilizar MySQL, pois tenho mais experiÃªncia e utilizo no dia a dia. No entanto, optei por MongoDB, conforme a proposta do desafio, para explorar uma abordagem NoSQL e facilitar o armazenamento dos dados flexÃ­veis do Open Food Facts.

### ConexÃ£o com o Banco de Dados  
O primeiro passo foi garantir a conexÃ£o com o banco de dados MongoDB (via Atlas).  
Com a conexÃ£o testada e validada, inseri manualmente alguns produtos na collection `products` para implementar rotas, controllers, models e migrations:

## ğŸ§© LÃ³gica de ImportaÃ§Ã£o de Produtos â€” OpenFoodFacts

### Problemas Iniciais

Inicialmente, a lÃ³gica de importaÃ§Ã£o foi implementada em um Ãºnico job que:
- Baixava o arquivo `.json.gz` completo;
- Descompactava para `.json`;
- Processava o conteÃºdo linha a linha.

PorÃ©m, os arquivos do OpenFoodFacts sÃ£o **extremamente grandes**, e esse processo causava **estouro de memÃ³ria** e **timeout** nos containers Docker com recursos limitados.

---

### âœ… SoluÃ§Ã£o Aplicada

Para resolver os problemas de performance e consumo de memÃ³ria, a lÃ³gica foi completamente refatorada com foco em **streaming sob demanda** e **processamento eficiente**:

#### 1. **Streaming direto do `.json.gz`**
- Utilizamos a biblioteca `GuzzleHttp` com a opÃ§Ã£o `['stream' => true]` para baixar os dados aos poucos.
- Os dados sÃ£o descompactados em tempo real usando `inflate_init(ZLIB_ENCODING_GZIP)` + `inflate_add()`, evitando salvar ou carregar o arquivo inteiro.
- Cada linha (em formato NDJSON) Ã© processada **conforme chega** via stream, utilizando `strpos()` para detectar quebras de linha.

#### 2. **Buffer e inserÃ§Ã£o em chunks**
- A cada 100 produtos, os dados sÃ£o inseridos em lote no banco via `Product::insert($buffer)`.
- Isso reduz o nÃºmero de queries e o uso de memÃ³ria.

#### 3. **Ajustes para ambiente restrito (Docker)**
- `set_time_limit(0)` para evitar timeouts em execuÃ§Ãµes longas.
- `ini_set('memory_limit', '2048M')` para garantir mais espaÃ§o (embora o uso real de memÃ³ria tenha sido drasticamente reduzido).
- Uso de logs (`Log::info`, `Log::warning`, `Log::error`) para monitoramento completo da importaÃ§Ã£o.

---

### ğŸ§  Arquitetura

- A lÃ³gica principal foi extraÃ­da para um **Service** (`OpenFoodFactsImportService`) reutilizÃ¡vel em qualquer lugar da aplicaÃ§Ã£o.
- Esse service pode ser usado:
  - Por um **Job assÃ­ncrono** (`ImportOpenFoodFactsJob`);
  - Por um **Controller** via rota HTTP (Ãºtil para testes manuais ou chamadas diretas).

---

### ğŸ“¦ Fila de Processamento

- A importaÃ§Ã£o Ã© feita por meio de **fila Laravel (`ShouldQueue`)**, o que mantÃ©m a aplicaÃ§Ã£o responsiva.
- Os jobs sÃ£o gerenciados por **supervisord** no ambiente Docker, garantindo execuÃ§Ã£o contÃ­nua.
- O Service pode opcionalmente limitar a importaÃ§Ã£o a um nÃºmero de registros (`$maxItems`), facilitando controle e testes.

---

### Agendamento de Tarefas  
A rotina de importaÃ§Ã£o Ã© executada diariamente Ã s **2h da manhÃ£** via Laravel Scheduler (`schedule:work`).  

- Em ambiente Docker, o `supervisord` Ã© responsÃ¡vel por manter o processo ativo e verificando a cada minuto.

### ConsideraÃ§Ãµes sobre o Docker  
Implementei uma configuraÃ§Ã£o Docker completa com:

- `Dockerfile`  
- `docker-compose.yml`  
- `supervisord.conf`

Tudo pronto para subir rapidamente o ambiente de desenvolvimento.

---

## âš ï¸ Pontos NÃ£o Implementados

Alguns requisitos do desafio foram considerados mas **nÃ£o implementados integralmente** por motivo de tempo ou escopo:

- âŒ **Endpoint de busca com ElasticSearch (ou similar)**  
  > A estrutura inicial para uma busca customizada foi iniciada, mas a integraÃ§Ã£o com ElasticSearch nÃ£o foi concluÃ­da.

- âŒ **Sistema de alerta em caso de falhas no Sync dos produtos**  
  > NÃ£o implementado. A aplicaÃ§Ã£o atualmente realiza o processo de importaÃ§Ã£o com filas e logs, mas sem alertas automatizados.

- âŒ **Esquema de seguranÃ§a com API Key nos endpoints**  
  > Planejado, porÃ©m nÃ£o implementado por falta de tempo hÃ¡bil. A API estÃ¡ atualmente aberta para facilitar os testes.

---

### âœ… Resultado Final

- ğŸ’¡ ImportaÃ§Ã£o **100% em stream**: sem salvar arquivos, sem estourar memÃ³ria.
- âš¡ï¸ ComeÃ§a a processar os dados imediatamente enquanto o arquivo ainda estÃ¡ baixando.
- ğŸ³ CompatÃ­vel com containers Docker com recursos limitados.
- â™»ï¸ Arquitetura desacoplada, reutilizÃ¡vel e preparada para escalar.

---
